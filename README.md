# Dialogue_Generation
Trained a transformer model dialogue agent, which will take one turn of conversational data and respond with an utterance to continue the conversation.

The dataset we will be utilizing is EmpatheticDialogues1. Each conversation in the dataset takes place between two humans. A prompt of a life event is given to ground the conversation (ex: you have just gotten a big promotion at work). An emotion is also given to influence how the person should feel about the conversation (ex: excited). An example conversation can be seen in the table below:

![Intro_Ex1](/Images/Intro_Ex1.png)

Here, we will be dealing with the conversation turns and emotion, not the prompt. You will be training a transformer model to respond to the next turn in a conversation. For example, for the conversation in the above table, your model will produce “That is quite an accomplishment and you should be proud!” given the input
“Thank you! I’ve ben trying to get it for a while now!”

## Transformer Model
We will be using pytorch’s nn.Transformer() library to train a transformer generator model. This model takes the most recent turn (past_turn) in a conversation and generates a response. The general transformer architecture can be seen below, reproduced from Vaswani et al.

The nn.Transformer package takes care of the transformer architecture in the grey boxes.
![Transformer](/Images/Transformer.PNG)

* self.model: this should be a call to nn.Transformer()4 with the necessary parameters.
* self.out: this should be the linear layer to produce the model’s output (the top grey Linear box in the Transformer diagram above).
* self.pos_encoder: this should be a TransformerPositionalEncoding for the input
* self.pos_decoder: this should be a TransformerPositionalEncoding for the output Forward components:
* src_masks: This should be a mask for the past_turn input. The value of this should be True for positions greater than the length of the input and False for positions less than the length. In other words, True values represent a PAD token for a position in the source sequence.
* tgt_masks: This is similar to src_masks but for the response.
* src: This should be the output of the input embedding and transformer-style positional encoding layers. In the diagram, this is the input that enters the grey box on the left.
* trg: This should be the output of the output embedding and positional encoding layers. In the diagram, this is the input that enters the grey box on the right.

## Decode
Once we have a trained model, let’s experiment with the responses it generates! During the training process, the data the model is trained on has the gold-standard response labeled. Loss is measured based on how likely the gold-standard token is from the model’s output probabilities.

Here, the generator is trained using the gold-standard input via a process called *Teacher Forcing*. When we decode multiple words in a row, using Teacher Forcing means that even if the model predicts a different word from the gold-standard data at time t, we pass in the gold-standard word as input at time t + 1, as opposed to passing the predicted word back to the model. For example, given a training sequence of words “I finally got promoted”, suppose that our model predicts “was” instead of “got” at position 3. Since we are using Teacher Forcing, when we move on to predict the word at position 4, we allow the model to attend to “got” at position 3 rather than “was”.

Once we train a model, we might want to test the model’s response on input that does not have corresponding gold-standard responses. We write code to extract a model’s response to an arbitrary input, tokenby token, by using id2string() and decode() methods. In  decode(), we inject some randomness, to explore other model responses than the greedy most-likely response.

The id2string() method is a helper method for decode(). This method takes a list of predicted words and returns the string representation of the id’s. Given the model’s vocabulary, this method creates an id2vocab dictionary which takes a list of model’s predicted word id’s and returns a list of vocabulary words (strings).

### Greedy Decoding
This process is described below as:
* Initialize an array (seq) to hold the model’s generated response. This should contain the START_DECODE token to begin with.
* Pass the past_turn and the seq to the model’s forward function, to generate the model’s next response
* Determine the most likely token as generated by the model (making sure the token corresponds to the current time step)
* Append the most likely token to *seq*
* If token is STOP_DECODE or *max_len* has been reached, break
